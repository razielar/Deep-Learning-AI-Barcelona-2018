{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Word Embedding\n",
    "## [IMDB dataset for binary sentiment classification](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "### Import modules:\n",
    "\n",
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution() #For tf 1.x, for tf 2 you don't need this\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:27:08.928133 4577232320 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0901 18:27:09.085484 4577232320 deprecation.py:323] From /anaconda3/envs/Tensorflow/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info= True, as_supervised= True)\n",
    "#tfds.list_builders() #to know all the datasets contained on tfds\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "### Create empty lists: \n",
    "training_sentences= []\n",
    "training_labels= []\n",
    "\n",
    "test_sentences= []\n",
    "test_labels= []\n",
    "\n",
    "for s,l in train_data: \n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(str(l.numpy()))\n",
    "\n",
    "for s,l in test_data:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(str(l.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig sentences: \n",
      " b'I have no idea what the other reviewer is talking about- this was a wonderful movie, and created a sense of the era that feels like time travel. The characters are truly young, Mary is a strong match for Byron, Claire is juvenile and a tad annoying, Polidori is a convincing beaten-down sycophant... all are beautiful, curious, and decadent... not the frightening wrecks they are in Gothic.<br /><br />Gothic works as an independent piece of shock film, and I loved it for different reasons, but this works like a Merchant and Ivory film, and was from my readings the best capture of what the summer must have felt like. Romantic, yes, but completely rekindles my interest in the lives of Shelley and Byron every time I think about the film. One of my all-time favorites.' \n",
      "\n",
      "Training labels: '0' is negative \n",
      " 1 \n",
      "\n",
      "Test sentences: \n",
      " b\"I've watched the movie actually several times. And what i want to say about it is the only thing that made this movie high rank was the Burak Altay's incredible performance, absolutely nothing but that. Not even those silly model named Deniz Akkaya and some of these popular names at times in the movie... Burak is definitely very talented i've seen a few jobs he made and been through. Even though this is kind of horror movie, he's doing really good job in comedy movies and also in dramas too. I bet most of you all saw Asmali Konak the movie and TV series, those two would go for an example... All i'm gonna say is you better watch out for the new works coming out from Burak then you'll see.. Keep the good work bro, much love..\" \n",
      "\n",
      "Test labels: '1' is positive \n",
      " 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainig sentences: \\n\",training_sentences[0], \"\\n\")\n",
    "print(\"Training labels: '0' is negative \\n\",training_labels[0], \"\\n\")\n",
    "print(\"Test sentences: \\n\", test_sentences[0], \"\\n\")\n",
    "print(\"Test labels: '1' is positive \\n\", test_labels[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: \n",
      " (25000,) \n",
      "\n",
      "Testing labels: \n",
      " (25000,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### When training my labels are expected to be numpy arrays: \n",
    "training_labels_final = np.array(training_labels)\n",
    "test_labels_final= np.array(test_labels)\n",
    "\n",
    "print(\"Training labels: \\n\", training_labels_final.shape, \"\\n\")\n",
    "print(\"Testing labels: \\n\", test_labels_final.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize our sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Hyperparameters:\n",
    "vocab_size= 10000\n",
    "embedding_dim= 16\n",
    "max_length= 120\n",
    "trunc_type= 'post'\n",
    "oov_tok= \"<OOV>\"\n",
    "\n",
    "tokenizer= Tokenizer(num_words= vocab_size, oov_token= oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index= tokenizer.word_index\n",
    "sequences= tokenizer.texts_to_sequences(training_sentences)\n",
    "padded= pad_sequences(sequences=sequences, maxlen= max_length, truncating=trunc_type)\n",
    "\n",
    "### -- Test sequences: \n",
    "testing_sequences= tokenizer.texts_to_sequences(test_sentences)\n",
    "testing_padded= pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n",
    "### --- Neural Network: \n",
    "#1) DEFINE THE MODEL: Sequential: defines a SEQUENCE of layers in the NN\n",
    "# 1.1) Embedding layer: the key to text sentiment analysis in TF; the result 2D array, row sentences and columns \n",
    "      #the 16th dimensions\n",
    "# 1.2) Flatten: we need to flat the 2D array from the embedding\n",
    "# 1.3) Dense: fully connected neurons, first hidden layer of 6 neurons\n",
    "# 1.4) Dense: one neuron\n",
    "\n",
    "model= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length), #the most important for NLP\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively we can use 'GlobalAveragePooling1D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length), #the most important for NLP\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
